{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll use a train-test partition as well as a validation set to get better insights about how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. From there, you'll define and compile the model like before. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Apply early stopping criteria with a neural network \n",
    "- Apply L1, L2, and dropout regularization on a neural network  \n",
    "- Examine the effects of training with more data on a neural network  \n",
    "\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "Run the following cell to import some of the libraries and classes you'll need in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:51:45.041351Z",
     "start_time": "2023-01-20T19:51:45.033257Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in the file `'Bank_complaints.csv'`. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:51:46.103229Z",
     "start_time": "2023-01-20T19:51:45.641286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:51:46.674251Z",
     "start_time": "2023-01-20T19:51:46.657235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools such as regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* Train - test split\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels \n",
    "\n",
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training neural networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your model's performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "- Generate a random sample of 10,000 observations using seed 123 for consistency of results. \n",
    "- Split this sample into `X` and `y` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:52:41.062095Z",
     "start_time": "2023-01-20T19:52:41.049009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Downsample the data\n",
    "df_sample = df.sample(10000, random_state=123)\n",
    "\n",
    "# Split the data into X and y\n",
    "y = df_sample.pop('Product')\n",
    "X = df_sample['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:52:41.385788Z",
     "start_time": "2023-01-20T19:52:41.376762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29561    I want to file a \" Bait and Switch '' Complain...\n",
       "26640    I am an account holder for my personal, busine...\n",
       "24498    they took my whole social security check i had...\n",
       "24594    This is in dispute of my Case number : XXXX. I...\n",
       "24249    My Bluebird card that i used for bill pay was ...\n",
       "                               ...                        \n",
       "16987    I went into a debt collection program in XXXX ...\n",
       "20005    I had an account with Walmart. The account was...\n",
       "28877    Wells Fargo continued to bill me for payroll s...\n",
       "9022     Per advice with a consumer credit counseling a...\n",
       "4555     I am being harrassed by Navient about a studen...\n",
       "Name: Consumer complaint narrative, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "- Split the data into training and test sets \n",
    "- Assign 1500 obervations to the test set and use 42 as the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:52:43.362432Z",
     "start_time": "2023-01-20T19:52:43.351996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set \n",
    "\n",
    "As mentioned in the previous lesson, it is good practice to set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test set can then be used to determine an unbiased perforance of the model. \n",
    "\n",
    "Run the cell below to further divide the training data into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:52:43.975766Z",
     "start_time": "2023-01-20T19:52:43.968735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing before building a neural network model. \n",
    "\n",
    "- Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "- Transform the training, validate, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:52:45.982702Z",
     "start_time": "2023-01-20T19:52:44.776237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "# Only keep the 2000 most common words \n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final)\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_matrix(X_train_final, mode='binary')\n",
    "X_val_tokens = tokenizer.texts_to_matrix(X_val, mode='binary')\n",
    "X_test_tokens = tokenizer.texts_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero. \n",
    "\n",
    "Transform the training, validate, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:53:12.495848Z",
     "start_time": "2023-01-20T19:53:12.472645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the product labels to numerical values\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final)\n",
    "\n",
    "\n",
    "y_train_lb = lb.transform(y_train_final)\n",
    "y_val_lb = lb.transform(y_val)\n",
    "y_test_lb = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:53:15.459711Z",
     "start_time": "2023-01-20T19:53:15.449205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 7)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_lb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:53:15.912430Z",
     "start_time": "2023-01-20T19:53:15.904783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 7)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(lb.transform(y_train_final))[:, :, 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:53:17.778270Z",
     "start_time": "2023-01-20T19:53:17.761239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 2000)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Baseline Model \n",
    "\n",
    "Rebuild a fully connected (Dense) layer network:  \n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions (since you are dealing with a multiclass problem, classifying the complaints into 7 classes) \n",
    "- Use a `'softmax'` activation function for the output layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:53:21.592033Z",
     "start_time": "2023-01-20T19:53:21.576066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 2000)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:53:23.391555Z",
     "start_time": "2023-01-20T19:53:23.369234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a baseline neural network model using Keras\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "baseline_model = models.Sequential()\n",
    "\n",
    "baseline_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "baseline_model.add(layers.Dense(25, activation='relu'))\n",
    "baseline_model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Compile this model with: \n",
    "\n",
    "- a stochastic gradient descent optimizer \n",
    "- `'categorical_crossentropy'` as the loss function \n",
    "- a focus on `'accuracy'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:53:24.616750Z",
     "start_time": "2023-01-20T19:53:24.600728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='SGD',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "- Train the model for 150 epochs in mini-batches of 256 samples \n",
    "- Include the `validation_data` argument to ensure you keep track of the validation loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:53:42.884765Z",
     "start_time": "2023-01-20T19:53:30.440716Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.9550 - accuracy: 0.1368 - val_loss: 1.9548 - val_accuracy: 0.1490\n",
      "Epoch 2/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9372 - accuracy: 0.1772 - val_loss: 1.9400 - val_accuracy: 0.1850\n",
      "Epoch 3/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9233 - accuracy: 0.2128 - val_loss: 1.9261 - val_accuracy: 0.2160\n",
      "Epoch 4/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9082 - accuracy: 0.2432 - val_loss: 1.9107 - val_accuracy: 0.2430\n",
      "Epoch 5/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8906 - accuracy: 0.2631 - val_loss: 1.8914 - val_accuracy: 0.2700\n",
      "Epoch 6/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8691 - accuracy: 0.2845 - val_loss: 1.8691 - val_accuracy: 0.2940\n",
      "Epoch 7/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8435 - accuracy: 0.3059 - val_loss: 1.8434 - val_accuracy: 0.3150\n",
      "Epoch 8/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8132 - accuracy: 0.3311 - val_loss: 1.8130 - val_accuracy: 0.3200\n",
      "Epoch 9/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7780 - accuracy: 0.3495 - val_loss: 1.7764 - val_accuracy: 0.3500\n",
      "Epoch 10/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7381 - accuracy: 0.3759 - val_loss: 1.7353 - val_accuracy: 0.3780\n",
      "Epoch 11/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6933 - accuracy: 0.4009 - val_loss: 1.6914 - val_accuracy: 0.4020\n",
      "Epoch 12/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6449 - accuracy: 0.4321 - val_loss: 1.6445 - val_accuracy: 0.4340\n",
      "Epoch 13/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5930 - accuracy: 0.4624 - val_loss: 1.5920 - val_accuracy: 0.4500\n",
      "Epoch 14/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5379 - accuracy: 0.4888 - val_loss: 1.5391 - val_accuracy: 0.4770\n",
      "Epoch 15/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4812 - accuracy: 0.5180 - val_loss: 1.4809 - val_accuracy: 0.5090\n",
      "Epoch 16/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4233 - accuracy: 0.5441 - val_loss: 1.4279 - val_accuracy: 0.5340\n",
      "Epoch 17/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3661 - accuracy: 0.5696 - val_loss: 1.3714 - val_accuracy: 0.5550\n",
      "Epoch 18/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3102 - accuracy: 0.5937 - val_loss: 1.3179 - val_accuracy: 0.5830\n",
      "Epoch 19/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2570 - accuracy: 0.6125 - val_loss: 1.2677 - val_accuracy: 0.5950\n",
      "Epoch 20/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2075 - accuracy: 0.6280 - val_loss: 1.2232 - val_accuracy: 0.6150\n",
      "Epoch 21/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1611 - accuracy: 0.6441 - val_loss: 1.1804 - val_accuracy: 0.6220\n",
      "Epoch 22/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1184 - accuracy: 0.6539 - val_loss: 1.1411 - val_accuracy: 0.6360\n",
      "Epoch 23/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0790 - accuracy: 0.6659 - val_loss: 1.1034 - val_accuracy: 0.6530\n",
      "Epoch 24/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0432 - accuracy: 0.6744 - val_loss: 1.0751 - val_accuracy: 0.6610\n",
      "Epoch 25/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0107 - accuracy: 0.6831 - val_loss: 1.0439 - val_accuracy: 0.6640\n",
      "Epoch 26/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9804 - accuracy: 0.6879 - val_loss: 1.0152 - val_accuracy: 0.6740\n",
      "Epoch 27/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9527 - accuracy: 0.6955 - val_loss: 0.9927 - val_accuracy: 0.6780\n",
      "Epoch 28/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9276 - accuracy: 0.7001 - val_loss: 0.9681 - val_accuracy: 0.6830\n",
      "Epoch 29/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9040 - accuracy: 0.7060 - val_loss: 0.9454 - val_accuracy: 0.6930\n",
      "Epoch 30/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8825 - accuracy: 0.7123 - val_loss: 0.9254 - val_accuracy: 0.6860\n",
      "Epoch 31/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8627 - accuracy: 0.7149 - val_loss: 0.9099 - val_accuracy: 0.7010\n",
      "Epoch 32/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8446 - accuracy: 0.7203 - val_loss: 0.8890 - val_accuracy: 0.7020\n",
      "Epoch 33/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8275 - accuracy: 0.7251 - val_loss: 0.8742 - val_accuracy: 0.7030\n",
      "Epoch 34/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8113 - accuracy: 0.7324 - val_loss: 0.8633 - val_accuracy: 0.7020\n",
      "Epoch 35/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7971 - accuracy: 0.7309 - val_loss: 0.8498 - val_accuracy: 0.7070\n",
      "Epoch 36/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7824 - accuracy: 0.7391 - val_loss: 0.8422 - val_accuracy: 0.7100\n",
      "Epoch 37/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7694 - accuracy: 0.7419 - val_loss: 0.8270 - val_accuracy: 0.7120\n",
      "Epoch 38/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7573 - accuracy: 0.7461 - val_loss: 0.8168 - val_accuracy: 0.7210\n",
      "Epoch 39/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7454 - accuracy: 0.7492 - val_loss: 0.8072 - val_accuracy: 0.7160\n",
      "Epoch 40/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7348 - accuracy: 0.7524 - val_loss: 0.7991 - val_accuracy: 0.7160\n",
      "Epoch 41/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7242 - accuracy: 0.7557 - val_loss: 0.7918 - val_accuracy: 0.7190\n",
      "Epoch 42/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7147 - accuracy: 0.7565 - val_loss: 0.7881 - val_accuracy: 0.7170\n",
      "Epoch 43/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7051 - accuracy: 0.7608 - val_loss: 0.7767 - val_accuracy: 0.7220\n",
      "Epoch 44/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.7616 - val_loss: 0.7688 - val_accuracy: 0.7270\n",
      "Epoch 45/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.7661 - val_loss: 0.7633 - val_accuracy: 0.7270\n",
      "Epoch 46/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6784 - accuracy: 0.7689 - val_loss: 0.7549 - val_accuracy: 0.7260\n",
      "Epoch 47/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6706 - accuracy: 0.7681 - val_loss: 0.7513 - val_accuracy: 0.7260\n",
      "Epoch 48/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.7711 - val_loss: 0.7452 - val_accuracy: 0.7320\n",
      "Epoch 49/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6557 - accuracy: 0.7747 - val_loss: 0.7411 - val_accuracy: 0.7300\n",
      "Epoch 50/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6481 - accuracy: 0.7781 - val_loss: 0.7405 - val_accuracy: 0.7280\n",
      "Epoch 51/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6416 - accuracy: 0.7792 - val_loss: 0.7356 - val_accuracy: 0.7330\n",
      "Epoch 52/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6340 - accuracy: 0.7817 - val_loss: 0.7290 - val_accuracy: 0.7360\n",
      "Epoch 53/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6282 - accuracy: 0.7817 - val_loss: 0.7256 - val_accuracy: 0.7320\n",
      "Epoch 54/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6217 - accuracy: 0.7848 - val_loss: 0.7216 - val_accuracy: 0.7350\n",
      "Epoch 55/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6152 - accuracy: 0.7859 - val_loss: 0.7191 - val_accuracy: 0.7320\n",
      "Epoch 56/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6096 - accuracy: 0.7916 - val_loss: 0.7136 - val_accuracy: 0.7380\n",
      "Epoch 57/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6035 - accuracy: 0.7908 - val_loss: 0.7140 - val_accuracy: 0.7330\n",
      "Epoch 58/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5980 - accuracy: 0.7931 - val_loss: 0.7072 - val_accuracy: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5927 - accuracy: 0.7933 - val_loss: 0.7046 - val_accuracy: 0.7370\n",
      "Epoch 60/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.7956 - val_loss: 0.7052 - val_accuracy: 0.7330\n",
      "Epoch 61/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7995 - val_loss: 0.7061 - val_accuracy: 0.7360\n",
      "Epoch 62/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5767 - accuracy: 0.8007 - val_loss: 0.6995 - val_accuracy: 0.7320\n",
      "Epoch 63/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5713 - accuracy: 0.8012 - val_loss: 0.6973 - val_accuracy: 0.7350\n",
      "Epoch 64/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.8049 - val_loss: 0.6935 - val_accuracy: 0.7410\n",
      "Epoch 65/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5617 - accuracy: 0.8048 - val_loss: 0.6915 - val_accuracy: 0.7450\n",
      "Epoch 66/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5563 - accuracy: 0.8080 - val_loss: 0.6933 - val_accuracy: 0.7350\n",
      "Epoch 67/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5517 - accuracy: 0.8093 - val_loss: 0.6896 - val_accuracy: 0.7380\n",
      "Epoch 68/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5470 - accuracy: 0.8141 - val_loss: 0.6882 - val_accuracy: 0.7370\n",
      "Epoch 69/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5425 - accuracy: 0.8124 - val_loss: 0.6856 - val_accuracy: 0.7380\n",
      "Epoch 70/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5381 - accuracy: 0.8149 - val_loss: 0.6822 - val_accuracy: 0.7420\n",
      "Epoch 71/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.8172 - val_loss: 0.6825 - val_accuracy: 0.7390\n",
      "Epoch 72/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5296 - accuracy: 0.8192 - val_loss: 0.6774 - val_accuracy: 0.7410\n",
      "Epoch 73/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5252 - accuracy: 0.8204 - val_loss: 0.6820 - val_accuracy: 0.7380\n",
      "Epoch 74/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.8231 - val_loss: 0.6782 - val_accuracy: 0.7400\n",
      "Epoch 75/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5165 - accuracy: 0.8233 - val_loss: 0.6762 - val_accuracy: 0.7390\n",
      "Epoch 76/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.8277 - val_loss: 0.6746 - val_accuracy: 0.7430\n",
      "Epoch 77/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5083 - accuracy: 0.8269 - val_loss: 0.6710 - val_accuracy: 0.7440\n",
      "Epoch 78/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.8279 - val_loss: 0.6711 - val_accuracy: 0.7420\n",
      "Epoch 79/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.8300 - val_loss: 0.6695 - val_accuracy: 0.7420\n",
      "Epoch 80/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4967 - accuracy: 0.8329 - val_loss: 0.6734 - val_accuracy: 0.7360\n",
      "Epoch 81/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.8337 - val_loss: 0.6715 - val_accuracy: 0.7440\n",
      "Epoch 82/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.8332 - val_loss: 0.6706 - val_accuracy: 0.7380\n",
      "Epoch 83/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.8353 - val_loss: 0.6671 - val_accuracy: 0.7380\n",
      "Epoch 84/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4811 - accuracy: 0.8373 - val_loss: 0.6646 - val_accuracy: 0.7430\n",
      "Epoch 85/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4780 - accuracy: 0.8381 - val_loss: 0.6643 - val_accuracy: 0.7410\n",
      "Epoch 86/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4737 - accuracy: 0.8411 - val_loss: 0.6660 - val_accuracy: 0.7410\n",
      "Epoch 87/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4710 - accuracy: 0.8408 - val_loss: 0.6613 - val_accuracy: 0.7400\n",
      "Epoch 88/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4672 - accuracy: 0.8413 - val_loss: 0.6679 - val_accuracy: 0.7380\n",
      "Epoch 89/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.8436 - val_loss: 0.6665 - val_accuracy: 0.7440\n",
      "Epoch 90/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4604 - accuracy: 0.8435 - val_loss: 0.6655 - val_accuracy: 0.7380\n",
      "Epoch 91/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.8464 - val_loss: 0.6634 - val_accuracy: 0.7390\n",
      "Epoch 92/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4536 - accuracy: 0.8471 - val_loss: 0.6589 - val_accuracy: 0.7420\n",
      "Epoch 93/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4502 - accuracy: 0.8491 - val_loss: 0.6634 - val_accuracy: 0.7410\n",
      "Epoch 94/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4470 - accuracy: 0.8504 - val_loss: 0.6572 - val_accuracy: 0.7400\n",
      "Epoch 95/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.8500 - val_loss: 0.6585 - val_accuracy: 0.7410\n",
      "Epoch 96/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4402 - accuracy: 0.8525 - val_loss: 0.6604 - val_accuracy: 0.7460\n",
      "Epoch 97/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8541 - val_loss: 0.6606 - val_accuracy: 0.7380\n",
      "Epoch 98/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4337 - accuracy: 0.8563 - val_loss: 0.6605 - val_accuracy: 0.7400\n",
      "Epoch 99/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4301 - accuracy: 0.8548 - val_loss: 0.6574 - val_accuracy: 0.7380\n",
      "Epoch 100/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4275 - accuracy: 0.8584 - val_loss: 0.6597 - val_accuracy: 0.7420\n",
      "Epoch 101/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.8579 - val_loss: 0.6585 - val_accuracy: 0.7390\n",
      "Epoch 102/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.8597 - val_loss: 0.6589 - val_accuracy: 0.7390\n",
      "Epoch 103/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4179 - accuracy: 0.8621 - val_loss: 0.6582 - val_accuracy: 0.7420\n",
      "Epoch 104/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4150 - accuracy: 0.8640 - val_loss: 0.6565 - val_accuracy: 0.7390\n",
      "Epoch 105/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8643 - val_loss: 0.6581 - val_accuracy: 0.7390\n",
      "Epoch 106/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8656 - val_loss: 0.6553 - val_accuracy: 0.7400\n",
      "Epoch 107/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4061 - accuracy: 0.8676 - val_loss: 0.6605 - val_accuracy: 0.7380\n",
      "Epoch 108/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4032 - accuracy: 0.8689 - val_loss: 0.6600 - val_accuracy: 0.7380\n",
      "Epoch 109/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3997 - accuracy: 0.8697 - val_loss: 0.6565 - val_accuracy: 0.7380\n",
      "Epoch 110/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8697 - val_loss: 0.6578 - val_accuracy: 0.7430\n",
      "Epoch 111/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8713 - val_loss: 0.6600 - val_accuracy: 0.7390\n",
      "Epoch 112/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8735 - val_loss: 0.6604 - val_accuracy: 0.7440\n",
      "Epoch 113/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3887 - accuracy: 0.8733 - val_loss: 0.6590 - val_accuracy: 0.7410\n",
      "Epoch 114/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3864 - accuracy: 0.8747 - val_loss: 0.6567 - val_accuracy: 0.7430\n",
      "Epoch 115/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3833 - accuracy: 0.8757 - val_loss: 0.6607 - val_accuracy: 0.7420\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3805 - accuracy: 0.8765 - val_loss: 0.6645 - val_accuracy: 0.7380\n",
      "Epoch 117/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3776 - accuracy: 0.8797 - val_loss: 0.6605 - val_accuracy: 0.7420\n",
      "Epoch 118/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8793 - val_loss: 0.6633 - val_accuracy: 0.7410\n",
      "Epoch 119/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8815 - val_loss: 0.6640 - val_accuracy: 0.7440\n",
      "Epoch 120/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3699 - accuracy: 0.8821 - val_loss: 0.6586 - val_accuracy: 0.7410\n",
      "Epoch 121/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8835 - val_loss: 0.6614 - val_accuracy: 0.7380\n",
      "Epoch 122/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8841 - val_loss: 0.6663 - val_accuracy: 0.7480\n",
      "Epoch 123/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3621 - accuracy: 0.8855 - val_loss: 0.6634 - val_accuracy: 0.7390\n",
      "Epoch 124/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3597 - accuracy: 0.8855 - val_loss: 0.6634 - val_accuracy: 0.7440\n",
      "Epoch 125/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8885 - val_loss: 0.6672 - val_accuracy: 0.7420\n",
      "Epoch 126/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8876 - val_loss: 0.6649 - val_accuracy: 0.7440\n",
      "Epoch 127/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3518 - accuracy: 0.8907 - val_loss: 0.6628 - val_accuracy: 0.7460\n",
      "Epoch 128/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8916 - val_loss: 0.6630 - val_accuracy: 0.7380\n",
      "Epoch 129/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8911 - val_loss: 0.6648 - val_accuracy: 0.7370\n",
      "Epoch 130/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.8941 - val_loss: 0.6684 - val_accuracy: 0.7440\n",
      "Epoch 131/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3423 - accuracy: 0.8945 - val_loss: 0.6642 - val_accuracy: 0.7420\n",
      "Epoch 132/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8953 - val_loss: 0.6718 - val_accuracy: 0.7400\n",
      "Epoch 133/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3374 - accuracy: 0.8960 - val_loss: 0.6669 - val_accuracy: 0.7400\n",
      "Epoch 134/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3351 - accuracy: 0.8981 - val_loss: 0.6702 - val_accuracy: 0.7450\n",
      "Epoch 135/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8967 - val_loss: 0.6732 - val_accuracy: 0.7400\n",
      "Epoch 136/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8991 - val_loss: 0.6804 - val_accuracy: 0.7390\n",
      "Epoch 137/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8981 - val_loss: 0.6738 - val_accuracy: 0.7410\n",
      "Epoch 138/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.9013 - val_loss: 0.6705 - val_accuracy: 0.7420\n",
      "Epoch 139/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.9021 - val_loss: 0.6719 - val_accuracy: 0.7440\n",
      "Epoch 140/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.9028 - val_loss: 0.6764 - val_accuracy: 0.7410\n",
      "Epoch 141/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.9039 - val_loss: 0.6741 - val_accuracy: 0.7470\n",
      "Epoch 142/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3171 - accuracy: 0.9047 - val_loss: 0.6731 - val_accuracy: 0.7430\n",
      "Epoch 143/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.9063 - val_loss: 0.6824 - val_accuracy: 0.7440\n",
      "Epoch 144/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.9071 - val_loss: 0.6763 - val_accuracy: 0.7450\n",
      "Epoch 145/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3103 - accuracy: 0.9067 - val_loss: 0.6746 - val_accuracy: 0.7410\n",
      "Epoch 146/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3080 - accuracy: 0.9093 - val_loss: 0.6819 - val_accuracy: 0.7450\n",
      "Epoch 147/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.9093 - val_loss: 0.6782 - val_accuracy: 0.7460\n",
      "Epoch 148/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3036 - accuracy: 0.9105 - val_loss: 0.6851 - val_accuracy: 0.7390\n",
      "Epoch 149/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3015 - accuracy: 0.9097 - val_loss: 0.6806 - val_accuracy: 0.7420\n",
      "Epoch 150/150\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.9123 - val_loss: 0.6842 - val_accuracy: 0.7390\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "baseline_model_val = baseline_model.fit(X_train_tokens, y_train_lb,\n",
    "                                       epochs=150, batch_size=256,\n",
    "                                       validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "The attribute `.history` (stored as a dictionary) contains four entries now: one per metric that was being monitored during training and validation. Print the keys of this dictionary for confirmation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:54:09.237583Z",
     "start_time": "2023-01-20T19:54:09.233068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the history attribute and store the dictionary\n",
    "baseline_model_val_dict = baseline_model_val.history\n",
    "\n",
    "# Print the keys\n",
    "baseline_model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:54:48.513212Z",
     "start_time": "2023-01-20T19:54:48.336210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 518us/step - loss: 0.2965 - accuracy: 0.9125\n",
      "----------\n",
      "Training Loss: 0.297 \n",
      "Training Accuracy: 0.913\n"
     ]
    }
   ],
   "source": [
    "results_train = baseline_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T19:55:08.297423Z",
     "start_time": "2023-01-20T19:55:08.240291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 530us/step - loss: 0.6125 - accuracy: 0.7887\n",
      "----------\n",
      "Test Loss: 0.612 \n",
      "Test Accuracy: 0.789\n"
     ]
    }
   ],
   "source": [
    "results_test = baseline_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print('----------')\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results \n",
    "\n",
    "Plot the loss versus the number of epochs. Be sure to include the training and the validation loss in the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss vs number of epochs with train and validation sets\n",
    "fig, ax = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a second plot comparing training and validation accuracy to the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs number of epochs with train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice an interesting pattern here? Although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss don't necessarily do the same. After a certain point, validation accuracy keeps swinging, which means that you're probably **overfitting** the model to the training data when you train for many epochs past a certain dropoff point. Let's tackle this now. You will now specify an early stopping point when training your model. \n",
    "\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "Overfitting neural networks is something you **_want_** to avoid at all costs. However, it's not possible to know in advance how many *epochs* you need to train your model on, and running the model multiple times with varying number of *epochs* maybe helpful, but is a time-consuming process. \n",
    "\n",
    "We've defined a model with the same architecture as above. This time specify an early stopping point when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model_2.add(layers.Dense(25, activation='relu'))\n",
    "model_2.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='SGD', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import `EarlyStopping` and `ModelCheckpoint` from `keras.callbacks` \n",
    "- Define a list, `early_stopping`: \n",
    "  - Monitor `'val_loss'` and continue training for 10 epochs before stopping \n",
    "  - Save the best model while monitoring `'val_loss'` \n",
    " \n",
    "> If you need help, consult [documentation](https://keras.io/callbacks/).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping and ModelCheckpoint\n",
    "\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `model_2`. Make sure you set the `callbacks` argument to `early_stopping`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best (saved) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best (saved) model\n",
    "\n",
    "saved_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this model to to calculate the training and test accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = saved_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = saved_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! Did you notice that the model didn't train for all 150 epochs? You reduced your training time. \n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance. \n",
    "\n",
    "## L2 Regularization \n",
    "\n",
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform. \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L2 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regularizers\n",
    "\n",
    "random.seed(123)\n",
    "L2_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "\n",
    "\n",
    "# Add another hidden layer\n",
    "\n",
    "\n",
    "# Add an output layer\n",
    "L2_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L2_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L2_model_val = L2_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training as well as the validation accuracy for both the L2 and the baseline models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 model details\n",
    "L2_model_dict = L2_model_val.history\n",
    "L2_acc_values = L2_model_dict['acc'] \n",
    "L2_val_acc_values = L2_model_dict['val_acc']\n",
    "\n",
    "# Baseline model\n",
    "baseline_model_acc = baseline_model_val_dict['acc'] \n",
    "baseline_model_val_acc = baseline_model_val_dict['val_acc']\n",
    "\n",
    "# Plot the accuracy for these models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, L2_acc_values, label='Training acc L2')\n",
    "ax.plot(epochs, L2_val_acc_values, label='Validation acc L2')\n",
    "ax.plot(epochs, baseline_model_acc, label='Training acc')\n",
    "ax.plot(epochs, baseline_model_val_acc, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy L2 vs regular')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better.  \n",
    "\n",
    "\n",
    "## L1 Regularization\n",
    "\n",
    "Now have a look at L1 regularization. Will this work better? \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L1 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "L1_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "\n",
    "\n",
    "# Add a hidden layer\n",
    "\n",
    "\n",
    "# Add an output layer\n",
    "L1_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L1_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L1_model_val = L1_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training as well as the validation accuracy for the L1 model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "L1_model_dict = L1_model_val.history\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, acc_values, label='Training acc L1')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n",
    "ax.set_title('Training & validation accuracy with L1 regularization')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy isn't still that good. Next, experiment with dropout regularization to see if it offers any advantages. \n",
    "\n",
    "\n",
    "## Dropout Regularization \n",
    "\n",
    "It's time to try another technique: applying dropout to layers. As discussed in the earlier lesson, this involves setting a certain proportion of units in each layer to zero. In the following cell: \n",
    "\n",
    "- Apply a dropout rate of 30% to the input layer \n",
    "- Add a first hidden layer with 50 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the first hidden layer \n",
    "- Add a second hidden layer with 25 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the second hidden layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⏰ This cell may take about a minute to run\n",
    "random.seed(123)\n",
    "dropout_model = models.Sequential()\n",
    "\n",
    "# Implement dropout to the input layer\n",
    "# NOTE: This is where you define the number of units in the input layer\n",
    "\n",
    "\n",
    "# Add the first hidden layer\n",
    "\n",
    "\n",
    "# Implement dropout to the first hidden layer \n",
    "\n",
    "\n",
    "# Add the second hidden layer\n",
    "\n",
    "\n",
    "# Implement dropout to the second hidden layer \n",
    "\n",
    "\n",
    "# Add the output layer\n",
    "dropout_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer='SGD', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "dropout_model_val = dropout_model.fit(X_train_tokens, \n",
    "                                      y_train_lb, \n",
    "                                      epochs=150, \n",
    "                                      batch_size=256, \n",
    "                                      validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again, and the training and test accuracy are very close!  \n",
    "\n",
    "## Bigger Data? \n",
    "\n",
    "Finally, let's examine if we can improve the model's performance just by adding more data. We've quadrapled the sample dataset from 10,000 to 40,000 observations, and all you need to do is run the code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigger_sample = df.sample(40000, random_state=123)\n",
    "\n",
    "X = df['Consumer complaint narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# Train-test split\n",
    "X_train_bigger, X_test_bigger, y_train_bigger, y_test_bigger = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size=6000, \n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Validation set\n",
    "X_train_final_bigger, X_val_bigger, y_train_final_bigger, y_val_bigger = train_test_split(X_train_bigger, \n",
    "                                                                                          y_train_bigger, \n",
    "                                                                                          test_size=4000, \n",
    "                                                                                          random_state=42)\n",
    "\n",
    "\n",
    "# One-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final_bigger)\n",
    "\n",
    "X_train_tokens_bigger = tokenizer.texts_to_matrix(X_train_final_bigger, mode='binary')\n",
    "X_val_tokens_bigger = tokenizer.texts_to_matrix(X_val_bigger, mode='binary')\n",
    "X_test_tokens_bigger = tokenizer.texts_to_matrix(X_test_bigger, mode='binary')\n",
    "\n",
    "# One-hot encoding of products\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final_bigger)\n",
    "\n",
    "y_train_lb_bigger = to_categorical(lb.transform(y_train_final_bigger))[:, :, 1]\n",
    "y_val_lb_bigger = to_categorical(lb.transform(y_val_bigger))[:, :, 1]\n",
    "y_test_lb_bigger = to_categorical(lb.transform(y_test_bigger))[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⏰ This cell may take several minutes to run\n",
    "random.seed(123)\n",
    "bigger_data_model = models.Sequential()\n",
    "bigger_data_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "bigger_data_model.add(layers.Dense(25, activation='relu'))\n",
    "bigger_data_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "bigger_data_model.compile(optimizer='SGD', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['acc'])\n",
    "\n",
    "bigger_data_model_val = bigger_data_model.fit(X_train_tokens_bigger,  \n",
    "                                              y_train_lb_bigger,  \n",
    "                                              epochs=150,  \n",
    "                                              batch_size=256,  \n",
    "                                              validation_data=(X_val_tokens_bigger, y_val_lb_bigger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = bigger_data_model.evaluate(X_train_tokens_bigger, y_train_lb_bigger)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = bigger_data_model.evaluate(X_val_tokens_bigger, y_val_lb_bigger)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs and no regularization technique, you were able to get both better test accuracy and loss. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance! \n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database \n",
    "\n",
    "\n",
    "## Summary  \n",
    "\n",
    "In this lesson, you built deep learning models using a validation set and used several techniques such as L2 and L1 regularization, dropout regularization, and early stopping to improve the accuracy of your models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
